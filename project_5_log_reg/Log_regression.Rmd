---
title: "Logit-regression additional project"
author: "Your collaborator"
date: "03/25/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_section: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(car)
library(ggExtra)
```

# Loading the data

The data analyzed is a freely available training dataset from the UCLA faculty of the Institute for Digital Research & Education (https://stats.idre.ucla.edu/stat/data/binary.csv).

According to the description, the dataset represents how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. 

The dataset has a binary response variable called **admit**. There are three predictor variables: **gre**, **gpa** and **rank**. We will treat the variables **gre** and **gpa** as continuous. The variable **rank** takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. 

Load and have a first look at the data

```{r}
setwd("~/BI/R/project5/")
df <- read.csv('binary.csv')
head(df)
```

Inspect the structure of the dataframe

```{r}
str(df)
```

# Data transformations

Transform continuous variables into numeric and obviously categorical into factor variable

```{r}
df$admit <- as.factor(df$admit)
df$rank <- as.factor(df$rank)
df$gre <- as.numeric(df$gre)
```

# Exploratory Data Analysis

Have a look for missing values present

```{r}
sum(is.na(df))
```

Explore summary of the data

```{r}
summary(df)
```

Perform some basic exploration - are GPA and GRE scores clearly correlative and how they linked with University ranking and an admission outcome?

```{r}
g1 <- ggplot(df, aes(gpa, gre, color=rank)) + 
  geom_point() +
  labs(y='GRE', x = 'GPA', title = 'GRE v GPA scores grouped by\nUniversity ranking') +
  theme_bw(base_size = 13) +
  theme(plot.title = element_text(hjust = 0.5))

ggMarginal(g1, type = "density", groupFill = TRUE, alpha = 0.5)
```

```{r}
g2 <- ggplot(df, aes(gpa, gre, color=admit)) + 
  geom_point() +
  labs(y='GRE', x = 'GPA', title = 'GRE v GPA scores grouped by\nadmission outcome') +
  theme_bw(base_size = 13)  +
  theme(plot.title = element_text(hjust = 0.5))

ggMarginal(g2, type = "density", groupFill = TRUE, alpha = 0.5)
```

As we can see, the GPA and GRE scores can be characterized as more or less interconnected and normally distributed across University ranking and an admission outcome variables

# Building a logit-model

Create and explore a full model 

```{r}
mod <- glm(admit ~ ., family = binomial(link = 'logit'), data = df)
Anova(mod)
```

Here we see all three variables to have significant impact on an admission outcome 

# Model diagnostics

## Linearity assumption

```{r}
mod_diag <- data.frame(.fitted = fitted(mod, type = 'response'),
                        .resid_p = resid(mod, type = 'pearson'))

ggplot(mod_diag, aes(y = .resid_p, x = .fitted)) + 
  geom_point() +
  geom_hline(yintercept = 0) +  
  geom_smooth(method = 'loess') +
  theme_bw(base_size = 13) 
```

As can be seen from the plot, the relationship between predictor variables and the logit of the outcome can be defined as linear

## Influential values

The most extreme values in the data can be examined by visualizing the Cook’s distance values. Here we label the top 3 largest values

```{r}
plot(mod, which = 4, id.n = 3)
```

## Overdispersion control

```{r}
overdisp_fun <- function(model) {
  rdf <- df.residual(model)  # Число степеней свободы N - p
  if (any(class(model) == 'negbin')) rdf <- rdf - 1 ## учитываем k в NegBin GLMM
  rp <- residuals(model,type='pearson') # Пирсоновские остатки
  Pearson.chisq <- sum(rp^2) # Сумма квадратов остатков, подчиняется Хи-квадрат распределению
  prat <- Pearson.chisq/rdf  # Отношение суммы квадратов остатков к числу степеней свободы
  pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE) # Уровень значимости
  c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)        # Вывод результатов
}

overdisp_fun(mod)
```

According to the results, the model has no overdispersion 

Let's explore the model coefficients in detail

```{r}
summary(mod)
```

Both **gre** and **gpa** are statistically significant, as are the three terms for **rank** 

*   For every one unit change in **gre**, the log odds of admission (versus non-admission) increases by 0.002
*   For a one unit increase in **gpa**, the log odds of being admitted to graduate school increases by 0.804
*   Having attended an undergraduate institution with **rank** of 2, versus an institution with a **rank** of 1, changes the log odds of admission by -0.675

# Making predictions

## Making imitative dataset

Build a new dataset with 400 observations (100 rep **rank**) holding **gpa** predictors values equal to its mean and varying **gre** scores

```{r}
new_df <- with(df, data.frame(gre = rep(seq(from = min(df$gre), 
                                            to = max(df$gre),
                                            length.out = 100), 4),                                         
                                        gpa = mean(df$gpa), 
                                        rank = factor(rep(1:4, each = 100))))
head(new_df)
```

## Making predictions

```{r}
new_df2 <- cbind(new_df, predict(mod, newdata = new_df, type = "link", se = TRUE))

new_df2 <- within(new_df2, {
  PredictedProb <- plogis(fit)
  LL <- plogis(fit - (1.96 * se.fit))
  UL <- plogis(fit + (1.96 * se.fit))
})

head(new_df2)
```

## Predictions visualization at the non-transformed scale

```{r}
ggplot(new_df2, aes(x = gre, y = PredictedProb)) +
  geom_ribbon(aes(ymin = LL, ymax = UL, fill = rank), alpha = 0.2) + 
  geom_line(aes(colour = rank), size = 1) +
  labs(y='Admission predicted\nprobability', x = 'GRE', title = 'Admission predicted\nprobability by GRE score') +
  theme_bw(base_size = 13)  +
  theme(plot.title = element_text(hjust = 0.5))
```
