---
title: "Down syndrome mouse proteins analysis"
author: "Your collaborator"

output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_section: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(car)
library(corrplot)
library(vegan)
library(rgl)
theme_set(theme_bw())
```

The analysis presented is an exploration of the Mice Protein Expression Data Set available with the open access at the https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression 

The data is a set of expression levels of 77 proteins measured in the cerebral cortex of 8 classes of control and Down syndrome mice exposed to context fear conditioning, a task used to assess associative learning.

# 1 Reading the data and learning its structure

In order to start calculations, please provide the full path to the directory where the dataset are stored (please convert the file to .csv extension if necessary) 

```{r, echo=FALSE}
df_start <- read.csv("/home/mks/BI/R/project3/Data_Cortex_Nuclear.csv")
```

Have the first look at the data

```{r}
str(df_start)
```

There are five columns that describe the dataset. All of them are to be converted to a factor variables for further use

```{r}
df_start$MouseID <- as.factor(df_start$MouseID)
df_start$Genotype <- as.factor(df_start$Genotype)
df_start$Treatment <- as.factor(df_start$Treatment)
df_start$Behavior <- as.factor(df_start$Behavior)
df_start$class <- as.factor(df_start$class) 
```

According to the description, the dataset consists of the expression levels of 77 proteins/protein modifications that produced detectable signals in the nuclear fraction of cortex. There are 38 control mice and 34 trisomic mice (Down syndrome), for a total of 72 mice. In the experiments, 15 measurements were registered of each protein per sample/mouse. Therefore, for control mice, there are 38x15, or 570 measurements, and for trisomic mice, there are 34x15, or 510 measurements. The dataset contains a total of 1080 measurements per protein. Each measurement can be considered as an independent sample/mouse.

The eight classes of mice are described based on features such as genotype, behavior and treatment. According to genotype, mice can be control or trisomic. According to behavior, some mice have been stimulated to learn (context-shock) and others have not (shock-context) and in order to assess the effect of the drug memantine in recovering the ability to learn in trisomic mice, some mice have been injected with the drug and others have not.

Let's verify and summarize the given description 

1. How many mouse were investigated in the experiments?

```{r}
df_modified_1 <- df_start %>% 
  separate(col = MouseID, into = c("Mouse_ID", "Observation"), sep = "_")
str(df_modified_1)
df_modified_1$Mouse_ID <- as.factor(df_modified_1$Mouse_ID)
df_modified_1$Observation <- as.factor(df_modified_1$Observation)

nlevels(df_modified_1$Mouse_ID)
```

2. What experimental groups can be distinguished? What is the number of these groups? 

Genotype: control (c) or trisomy (t)

```{r}
nlevels(df_modified_1$Genotype)
levels(df_modified_1$Genotype)
```

Behavior: context-shock (CS) or shock-context (SC)

```{r}
nlevels(df_modified_1$Behavior)
levels(df_modified_1$Behavior)
```

Treatment type: memantine (m) or saline (s)

```{r}
nlevels(df_modified_1$Treatment)
levels(df_modified_1$Treatment)
```

Class: c-CS-s, c-CS-m, c-SC-s, c-SC-m, t-CS-s, t-CS-m, t-SC-s, t-SC-m

```{r}
nlevels(df_modified_1$class)
levels(df_modified_1$class)
```

In summary, the following classes of mouse are in the dataset

* c-CS-s: control mice, stimulated to learn, injected with saline (9 mice)
* c-CS-m: control mice, stimulated to learn, injected with memantine (10 mice)
* c-SC-s: control mice, not stimulated to learn, injected with saline (9 mice)
* c-SC-m: control mice, not stimulated to learn, injected with memantine (10 mice)

* t-CS-s: trisomy mice, stimulated to learn, injected with saline (7 mice)
* t-CS-m: trisomy mice, stimulated to learn, injected with memantine (9 mice)
* t-SC-s: trisomy mice, not stimulated to learn, injected with saline (9 mice)
* t-SC-m: trisomy mice, not stimulated to learn, injected with memantine (9 mice)

3. Are the identified groups  balanced?

```{r}
df_modified_1 %>%
  group_by(class) %>%
  count()

df_modified_1 %>%
  filter(Observation == 1) %>% 
  group_by(class) %>%
  count()
```

4. What is the number of missing values in the dataset?

```{r}
sum(is.na(df_start))
```

Where the missing values are located? 

```{r}
NA_by_col <- sapply(df_start[,-c(1,79,80,81,82)], function(x) sum(is.na(x)))
plot(NA_by_col)
```

As we can see, the vast majority of missing data is concentrated at the last columns of the dataset. Let's identify this columns via simple pie chart for the number of missing values

```{r}
pie(NA_by_col, labels=names(NA_by_col)) 
```

# 2 BDNF_N level by mouse experimental class 

First, visualize all available BDNF protein levels by class

```{r}
boxplot(BDNF_N ~ class, data=df_start)
```

From the first look one can assume the distribution of the protein production across classes is more or less simular. From the EDA above we know the groups are not balansed ideally, but could be considered as compareable. Having this in mind we could further use simple ANOVA to test is there any difference in the protein production across groups as follows

```{r}
fit <- aov(BDNF_N ~ class, data=df_start)
summary(fit)
```

And even test what groups differ in a pair-wise manner after adjustment 

```{r}
TukeyHSD(fit)
```

However, this line of view has obvious limitations. Turning back to the two assumptions to apply ANOVA we should not turn a blind eye to unequal sized of the groups as wells as assume variations are similar. Indeed, if one again summarized 15 measure repeats for all mouse and rebuild the boxplot, the variation would look different

```{r}
df_by_mouse_id <- df_modified_1 %>% 
  group_by(class, Mouse_ID) %>% 
  summarise(
    count = n(),
    mean = mean(BDNF_N, na.rm = TRUE),
    sd = sd(BDNF_N, na.rm = TRUE),
    median = median(BDNF_N, na.rm = TRUE),
    IQR = IQR(BDNF_N, na.rm = TRUE)) 

df_by_mouse_id

boxplot(mean ~ class, data=df_by_mouse_id)
```

Again, examine the number of mouse in the classes

```{r}
df_by_mouse_id %>% 
  count(class)
```

Hense, to be more correct one should apply non-standard testing with the use of the Kruskal-Wallis Rank Sum Test

```{r}
kruskal.test(mean ~ class, data=df_by_mouse_id)
```

As we can see, the p-value here is drammatically different from the oblained above via classical ANOVA. And the only reason for this is the absence of difference of BDNF production among almost all classes

```{r}
pairwise.wilcox.test(df_by_mouse_id$mean, df_by_mouse_id$class,
                     p.adjust.method = "BH")
```

# 3 A linear model for ERBB4 production

## Building and optimisation of a model

First, clean the dataframe from the columns with large missing values number identified in the EDA
```{r}
str(df_modified_1)
df_3 <- df_modified_1[,-c(2, 71, 72, 74, 76, 77, 78)]
```

Having results of the analysis of BDNF production in mind, hereafter the same approach of mouse data summarization has been used

```{r}
str(df_3)
df_3a <- df_3 %>% 
  group_by(class, Mouse_ID) %>% 
  summarise_all(funs(mean = mean(., na.rm = TRUE)))
```

Reformat the dataframe further to contain only proteins production information to build a model and exclude the remaining missing observations

```{r}
str(df_3a)
df_by_mouse_id_all <- df_3a[,-c(1,2,74,75,76)]

str(df_by_mouse_id_all)
sum(is.na(df_by_mouse_id_all))
df_by_mouse_id_all <- drop_na(df_by_mouse_id_all)
```

To build a linear model one should exclude from the analysis highly correlative variables. Let's look at them all

```{r}
res_cor <- cor(df_by_mouse_id_all)
corrplot(res_cor, type = "lower", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

As one can see, there is a huge multicollinearity issue with the data. Via application of the following code chunk we can remove columns that have correlation coefficients above 0.9 from the dataset

```{r}
res_cor[upper.tri(res_cor)] <- 0
diag(res_cor) <- 0

df_by_mouse_id_all_upd <- df_by_mouse_id_all[,!apply(res_cor,2,function(x) any(abs(x) > 0.90))]
```

So we are ready to create the first dirty linear model to predict ERBB4 production

```{r}
fit_full <- lm(ERBB4_N_mean ~ ., data = df_by_mouse_id_all_upd)
summary(fit_full)
```

Further we can apply backward selection of variables in the model to select the meaningful ones

```{r}
step(fit_full, direction = "backward")
```

The last iteration of the step function produced more optimal number of variables that we can select further manually

```{r}
fit_red1 <- lm(formula = ERBB4_N_mean ~ BDNF_N_mean + NR2A_N_mean + pCAMKII_N_mean + 
                pCREB_N_mean + pJNK_N_mean + pMEK_N_mean + pNR1_N_mean + 
                pNR2A_N_mean + pNR2B_N_mean + pPKCAB_N_mean + pRSK_N_mean + 
                AKT_N_mean + BRAF_N_mean + CAMKII_N_mean + CREB_N_mean + 
                ELK_N_mean + ERK_N_mean + GSK3B_N_mean + MEK_N_mean + RSK_N_mean + 
                Bcatenin_N_mean + pMTOR_N_mean + DSCR1_N_mean + AMPKA_N_mean + 
                NR2B_N_mean + RAPTOR_N_mean + TIAM1_N_mean + P70S6_N_mean + 
                pGSK3B_N_mean + pPKCG_N_mean + CDK5_N_mean + S6_N_mean + 
                ADARB1_N_mean + RRP1_N_mean + BAX_N_mean + nNOS_N_mean + 
                Tau_N_mean + GFAP_N_mean + GluR3_N_mean + GluR4_N_mean + 
                IL1B_N_mean + P3525_N_mean + pCASP9_N_mean + PSD95_N_mean + 
                SNCA_N_mean + pGSK3B_Tyr216_N_mean + SHH_N_mean + SYP_N_mean + 
                CaNA_N_mean, data = df_by_mouse_id_all_upd)
summary(fit_red1)
vif(fit_red1)
drop1(fit_red1, test = "F")
```

Remove several variables again

```{r}
fit_red2 <- update(fit_red1, .~. - pCREB_N_mean
                                 - pJNK_N_mean
                                 - pNR2A_N_mean
                                 - ELK_N_mean
                                 - ERK_N_mean
                                 - MEK_N_mean
                                 - Bcatenin_N_mean
                                 - CDK5_N_mean
                                 - S6_N_mean
                                 - ADARB1_N_mean
                                 - nNOS_N_mean
                                 - pCASP9_N_mean
                                 - pGSK3B_Tyr216_N_mean
                                 - SHH_N_mean
                                 - SYP_N_mean)

summary(fit_red2)
vif(fit_red2)
drop1(fit_red2, test = "F")
```

And again

```{r}
fit_red3 <- update(fit_red2, .~. - BDNF_N_mean
                   - CAMKII_N_mean
                   - pMTOR_N_mean
                   - TIAM1_N_mean
                   - P70S6_N_mean
                   - BAX_N_mean
                   - SNCA_N_mean)

summary(fit_red3)
vif(fit_red3)
drop1(fit_red3, test = "F")
```

Remove more variables

```{r}
fit_red4 <- update(fit_red3, .~. - pCAMKII_N_mean ###################################
                   - NR2B_N_mean
                   - AKT_N_mean
                   - pRSK_N_mean
                   - NR2A_N_mean)

summary(fit_red4)
vif(fit_red4)
drop1(fit_red4, test = "F")
```

And finally remove some more variables

```{r}
fit_red5 <- update(fit_red4, .~. - GFAP_N_mean ###################
                   - GluR4_N_mean
                   - AMPKA_N_mean
                   - DSCR1_N_mean
                   - GluR3_N_mean
                   - RAPTOR_N_mean
                   - CREB_N_mean
                   - pNR2B_N_mean
                   - pNR1_N_mean
                   - RSK_N_mean)
summary(fit_red5)
vif(fit_red5)
drop1(fit_red5, test = "F")
```

## Diagnostics of the optimised model

Plotting residuals, Cook’s distance and quantile plots 

```{r}
fit_red5_diag <- data.frame(fortify(fit_red5), df_by_mouse_id_all_upd[, c(11,14,19,37,38,43,47,51,52,54,61)])

ggplot(data = fit_red5_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")
```

```{r}
ggplot(fit_red5_diag, aes(x = 1:nrow(fit_red5_diag), y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red")
```

```{r}
qqPlot(fit_red5_diag$.stdresid)
```

Overall, giving the starting number of variables (77 proteins) and their number in the optimized model (12) as well as metrics and findings of diagnostics we can consider the optimized model to describe ERBB4 production pretty well

# 4 PCA ordination

One robust approach to describe a complex multidimensional data is the dimensional reduction via PCA. Let's run the ordination and explore the first 6 components that in summary descrides around 93% of the data

```{r}
mouse_pca <- rda(df_by_mouse_id_all)
head(summary(mouse_pca))
```

Create classical biplot with simmetrical scaling

```{r}
biplot(mouse_pca)
```

Create biplot with variables weights

```{r}
biplot(mouse_pca, scaling = "species", display = "species")
```

Create ordination biplot 

```{r}
biplot(mouse_pca, scaling = "sites", display = "sites")
```

Create ordination biplot via ggplot2

```{r}

df_3ab <- df_3a[-c(13,38),]
df_scores <- data.frame(df_3ab,
                        scores(mouse_pca, display = "sites", choices = c(1, 2, 3), scaling = "sites"))

ggplot(df_scores, aes(x = PC1, y = PC2)) + 
  geom_point(aes( color = class), alpha = 0.5) +
  coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) + ggtitle(label = "Ординация в осях главных компонент") + theme_bw()
```

Number of eigenvals

```{r}
eigenvals(mouse_pca)
```

Number of bstick coefficients

```{r}
bstick(mouse_pca)
```

Screeplot

```{r}
screeplot(mouse_pca, type = "lines", bstick = TRUE)
```

Graphical representation of the % of explained information

```{r}
pca_summary <- summary(mouse_pca)
pca_result <- as.data.frame(pca_summary$cont)
plot_data <- as.data.frame(t(pca_result[c("Proportion Explained"),]))
plot_data$component <- rownames(plot_data)
plot_data <- plot_data %>% 
  separate(col = component, into = c("inf", "PC_number"), sep = "PC")
str(plot_data)
plot_data$PC_number <- as.numeric(plot_data$PC_number) 

ggplot(plot_data, aes(PC_number,`Proportion Explained`)) + geom_bar(stat = "identity") + theme_bw()
```

Variables weights across princinal components

```{r}
sc <- scores(mouse_pca, display = "species", 
       choices = c(1, 2, 3), scaling = 0)
```
